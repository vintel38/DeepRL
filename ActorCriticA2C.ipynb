{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad81d12-e52b-4180-99c9-788c8ac3bbe5",
   "metadata": {},
   "source": [
    "# Actor Critic pour résoudre l'environnement OpenAI Gym Cartpole\n",
    "\n",
    "Copie expliquée de la page https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879aa093-7a16-405e-8d7d-ce6e7943ee20",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "L'algorithme Actor Critic est une famille d'algorithme de Reinforcement Learning qui divise la tâche de sélection de l'action en deux parties à savoir :\n",
    "- une partie (Critic) qui évalue l'intérêt ou la valeur de l'action au regard de l'environnement et du système considéré\n",
    "- une partie (Actor) qui en utilisant les informations générées par le Critic choisit effectivement l'action pour l'état présent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614fae2-3cc2-4225-a159-89a2d9069f32",
   "metadata": {},
   "source": [
    "***\n",
    "On commence par importer les packages nécessaires au programme et on met en place l'environnement qui sera utilisé par l'algorithme. On définit ensuite quelques variables qui seront utiles au problème. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dede8173-0eb3-4bf7-b9be-4345d814544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1938ba6a-753e-432c-9a6a-7fa6deefe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation = 'relu')(inputs)\n",
    "action = layers.Dense(num_actions, activation = \"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = [action, critic])\n",
    "\n",
    "# actor and critic tend to share initial layer weight to fasten convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732d701b-48e6-49ef-a6bc-babc9acd5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Running reward 13.238667661672265 at episode 10\n",
      "Running reward 27.384802149544235 at episode 20\n",
      "Running reward 30.398359697927035 at episode 30\n",
      "Running reward 34.4238415627547 at episode 40\n",
      "Running reward 31.575467282025855 at episode 50\n",
      "Running reward 31.938062653795036 at episode 60\n",
      "Running reward 43.2134828329286 at episode 70\n",
      "Running reward 43.576382205728244 at episode 80\n",
      "Running reward 33.268026518601395 at episode 90\n",
      "Running reward 29.718090561352323 at episode 100\n",
      "Running reward 29.565569697173785 at episode 110\n",
      "Running reward 33.42220111699168 at episode 120\n",
      "Running reward 37.29445342808401 at episode 130\n",
      "Running reward 45.44014937238755 at episode 140\n",
      "Running reward 62.07309623023724 at episode 150\n",
      "Running reward 70.94008678616326 at episode 160\n",
      "Running reward 89.39559268174861 at episode 170\n",
      "Running reward 95.27518657298502 at episode 180\n",
      "Running reward 101.17339158852432 at episode 190\n",
      "Running reward 93.46174549912742 at episode 200\n",
      "Running reward 86.53276463397023 at episode 210\n",
      "Running reward 102.82172317309082 at episode 220\n",
      "Running reward 102.86813983000272 at episode 230\n",
      "Running reward 86.67330052619242 at episode 240\n",
      "Running reward 82.21455435826816 at episode 250\n",
      "Running reward 90.27968538447408 at episode 260\n",
      "Running reward 97.66763839878824 at episode 270\n",
      "Running reward 116.86970609081968 at episode 280\n",
      "Running reward 109.12434595754974 at episode 290\n",
      "Running reward 119.92304044602702 at episode 300\n",
      "Running reward 137.50961037886958 at episode 310\n",
      "Running reward 150.00623918492082 at episode 320\n",
      "Running reward 153.30115185650334 at episode 330\n",
      "Running reward 151.44142395062292 at episode 340\n",
      "Running reward 153.07672047440752 at episode 350\n",
      "Running reward 150.99109022841796 at episode 360\n",
      "Running reward 156.49725721507298 at episode 370\n",
      "Running reward 141.67842922495134 at episode 380\n",
      "Running reward 132.0513615143582 at episode 390\n",
      "Running reward 121.53150494236684 at episode 400\n",
      "Running reward 123.54826323253947 at episode 410\n",
      "Running reward 130.69340816676564 at episode 420\n",
      "Running reward 144.46123122008942 at episode 430\n",
      "Running reward 162.40304622154628 at episode 440\n",
      "Running reward 177.48931497000171 at episode 450\n",
      "Running reward 180.96452134497957 at episode 460\n",
      "Running reward 167.12160026110502 at episode 470\n",
      "Running reward 180.31448757327806 at episode 480\n",
      "Running reward 182.0233090422854 at episode 490\n",
      "Running reward 163.6215657281616 at episode 500\n",
      "Running reward 178.09283772784997 at episode 510\n",
      "Running reward 186.88337271377438 at episode 520\n",
      "Running reward 192.14659072551456 at episode 530\n",
      "Solved at episode 539!\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history=[]\n",
    "critic_value_history=[]\n",
    "reward_history=[]\n",
    "running_reward=0\n",
    "episode_count=0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward=0\n",
    "    with tf.GradientTape() as tape: # A retravailler\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render() # Si on veux le rendu\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            \n",
    "            # pour l'état présent, renvoie les sorties des deux 2N Actor et Critic\n",
    "            # stocke l'avantage pour des calculs ultérieurs hors de la boucle\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0,0])\n",
    "            # print(action_probs)\n",
    "            # print(critic_value)\n",
    "            # Policy : donne l'action selon les actions possibles et la distribution d'actions donnée par l'actor\n",
    "            # puis stocke cette action dans l'historique\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0,action]))\n",
    "            \n",
    "            # Joue l'environnement pour l'état présent et l'action précédemment choisie\n",
    "            # puis stocke la récompense dans l'historique\n",
    "            # Si done, alors le pendule est tombé et la simulation n'est plus intéressante.\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            reward_history.append(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        running_reward = 0.05*episode_reward + (1-0.05)*running_reward\n",
    "        \n",
    "        # Calcul du retour attendu pour l'état présent \n",
    "        # le calcul \n",
    "        \n",
    "        returns = []\n",
    "        discounted_sum=0\n",
    "        for r in reward_history[::-1]:\n",
    "            discounted_sum = r + gamma*discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns))/(np.std(returns)+eps)\n",
    "        returns = returns.tolist()\n",
    "        \n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            adv = ret - value \n",
    "            actor_losses.append(-log_prob * adv)\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret,0))  )\n",
    "            \n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        # print(loss_value)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        reward_history.clear()\n",
    "        \n",
    "        episode_count += 1\n",
    "        if episode_count%10==0:\n",
    "            print(\"Running reward {} at episode {}\".format(running_reward, episode_count))\n",
    "        \n",
    "        if running_reward > 195: # Condition to consider the task solved \n",
    "            print(\"Solved at episode {}!\".format(episode_count))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20056ed4-5942-4b39-ac89-c1581c5d811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\envs\\OpenAI\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: A2C\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('A2C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdbaef-80cc-47e3-a340-df22abae11ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa20b2a-24c4-4b85-8e27-44da255a7d5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-829de482a7e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# episode_reward=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# A retravailler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "reward_history=[]\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    # episode_reward=0\n",
    "    with tf.GradientTape() as tape: # A retravailler\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            env.render() # Si on veux le rendu\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            \n",
    "            # pour l'état présent, renvoie les sorties des deux 2N Actor et Critic\n",
    "            # stocke l'avantage pour des calculs ultérieurs hors de la boucle\n",
    "            action_probs, critic_value = model(state)\n",
    "            # critic_value_history.append(critic_value[0,0])\n",
    "            # print(action_probs)\n",
    "            # print(critic_value)\n",
    "            # Policy : donne l'action selon les actions possibles et la distribution d'actions donnée par l'actor\n",
    "            # puis stocke cette action dans l'historique\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            # action_probs_history.append(tf.math.log(action_probs[0,action]))\n",
    "            \n",
    "            # Joue l'environnement pour l'état présent et l'action précédemment choisie\n",
    "            # puis stocke la récompense dans l'historique\n",
    "            # Si done, alors le pendule est tombé et la simulation n'est plus intéressante.\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            reward_history.append(reward)\n",
    "            # episode_reward += reward\n",
    "            \n",
    "            # if done:\n",
    "                # break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738760f-ef87-4ab5-b9a5-bc1c409cd5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
