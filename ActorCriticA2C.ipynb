{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad81d12-e52b-4180-99c9-788c8ac3bbe5",
   "metadata": {},
   "source": [
    "# Actor Critic pour résoudre l'environnement OpenAI Gym Cartpole\n",
    "\n",
    "Copie expliquée de la page https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879aa093-7a16-405e-8d7d-ce6e7943ee20",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "L'algorithme Actor Critic est une famille d'algorithme de Reinforcement Learning qui divise la tâche de sélection de l'action en deux parties à savoir :\n",
    "- une partie (Critic) qui évalue l'intérêt ou la valeur de l'action au regard de l'environnement et du système considéré\n",
    "- une partie (Actor) qui en utilisant les informations générées par le Critic choisit effectivement l'action pour l'état présent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614fae2-3cc2-4225-a159-89a2d9069f32",
   "metadata": {},
   "source": [
    "***\n",
    "On commence par importer les packages nécessaires au programme et on met en place l'environnement qui sera utilisé par l'algorithme. On définit ensuite quelques variables qui seront utiles au problème. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede8173-0eb3-4bf7-b9be-4345d814544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keyboard\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dd633-9018-49ea-9f31-b230e5b8b974",
   "metadata": {},
   "source": [
    "***\n",
    "On définit les deux Neural Networks qui représentent l'acteur et le critique. Le critique prend en entrée le vecteur issu de l'observation de l'état de l'environnement et renvoie en sortie la valeur calculée de l'état actuel. L'acteur prend également en entrée le vecteur observation mais en sortie renvoie une probabilité d'action pour chaque action possible. On verra par la suite que l'acteur se nourrit de la valeur estimée par le critique pour générer la distribution probabilistique d'exécuter les actions. En général, comme c'est donc fait dans l'implémentation suivante, le critique et l'acteur partage quelques-unes des premières couches de neurones pour accélérer la convergence du modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938ba6a-753e-432c-9a6a-7fa6deefe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation = 'relu')(inputs)\n",
    "action = layers.Dense(num_actions, activation = \"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = [action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a46d4-9ed5-443d-bb1c-84833205d7a7",
   "metadata": {},
   "source": [
    "***\n",
    "L'environnement est dit résolu lorsque la récompense totale moyenne atteint 195 pour 100 essais consécutifs. Aussi, on va effectuer un certains nombres d'itérations du programme non défini tant que la condition n'est pas atteinte. \n",
    "\n",
    "On commence donc par initialiser l'environnement avec la fonction habituelle `env.reset()` et créer des listes vides qui vont nous permettre de stocker les valeurs successives de variables. La première étape consiste à utiliser les réseaux acteur et critique pour déterminer une distribution probabilistique sur les actions possibles. Puis on choisit aléatoirement une action dans cette distribution que l'on applique à l'environnement en utilisant la fonction `env.step()`. Les éléments renvoyés sont stockés et on peut vérifier si l'exécution de l'environnement est terminé avec la variable `done`. \n",
    "\n",
    "A l'extérieur de l'exécution de l'épisode, on cumule la récompense avec celle des autres épisodes puis on calcule le retour attendu durant l'épisode qui vient d'être calculé. Le calcul du retour s'effectue avec la séquence de récompense dans l'ordre [inverse](https://stackoverflow.com/questions/31633635/what-is-the-meaning-of-inta-1-in-python). Le retour est alors normalisé et reformaté. Les pertes attribuées à l'acteur et au critique sont alors calculées durant tout l'épisode qui vient d'être accompli et leur somme globale est utilisée pour effectuer une descente de gradient sur les deux réseaux de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d701b-48e6-49ef-a6bc-babc9acd5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history=[]\n",
    "critic_value_history=[]\n",
    "reward_history=[]\n",
    "running_reward=0\n",
    "episode_count=0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    episode_reward=0\n",
    "    with tf.GradientTape() as tape: # A retravailler\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render() # Si on veux le rendu\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            \n",
    "            # pour l'état présent, renvoie les sorties des deux 2N Actor et Critic\n",
    "            # stocke l'avantage pour des calculs ultérieurs hors de la boucle\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0,0])\n",
    "            # print(action_probs)\n",
    "            # print(critic_value)\n",
    "            # Policy : donne l'action selon les actions possibles et la distribution d'actions donnée par l'actor\n",
    "            # puis stocke cette action dans l'historique\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0,action]))\n",
    "            \n",
    "            # Joue l'environnement pour l'état présent et l'action précédemment choisie\n",
    "            # puis stocke la récompense dans l'historique\n",
    "            # Si done, alors le pendule est tombé et la simulation n'est plus intéressante.\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            reward_history.append(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        running_reward = 0.05*episode_reward + (1-0.05)*running_reward\n",
    "        \n",
    "        # Calcul du retour attendu pour l'état présent \n",
    "        # le calcul \n",
    "        \n",
    "        returns = []\n",
    "        discounted_sum=0\n",
    "        for r in reward_history[::-1]: \n",
    "            discounted_sum = r + gamma*discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns))/(np.std(returns)+eps)\n",
    "        returns = returns.tolist()\n",
    "        \n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            adv = ret - value \n",
    "            actor_losses.append(-log_prob * adv)\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret,0))  )\n",
    "            \n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        # print(loss_value)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        reward_history.clear()\n",
    "        \n",
    "        episode_count += 1\n",
    "        if episode_count%10==0:\n",
    "            print(\"Running reward {} at episode {}\".format(running_reward, episode_count))\n",
    "        \n",
    "        if running_reward > 195: # Condition to consider the task solved \n",
    "            print(\"Solved at episode {}!\".format(episode_count))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811ed3e-ef0c-4b76-9c1f-0bb54e56b17e",
   "metadata": {},
   "source": [
    "***\n",
    "Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20056ed4-5942-4b39-ac89-c1581c5d811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('A2C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e07f66-dbef-45cf-8c40-8f10cf8c0d4b",
   "metadata": {},
   "source": [
    "***\n",
    "A partir de l'environnement sauvegardé, l'environnement est joué pour 10000 épisodes. On réutilise donc les deux réseaux de neurones définis dans l'entraînement et l'action choisie est toujours issue de la distribution de probabilité de l'acteur. \n",
    "\n",
    "JOUER l'environnement avec l'entrainement effectué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a497e8-eec6-44ed-a1b1-7f582bc5870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keyboard\n",
    "\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa20b2a-24c4-4b85-8e27-44da255a7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history=[]\n",
    "num_actions = 2\n",
    "\n",
    "model = keras.models.load_model(\"A2C\")\n",
    "model.summary()\n",
    "\n",
    "# while True:\n",
    "state = env.reset()\n",
    "    # episode_reward=0\n",
    "with tf.GradientTape() as tape: # A retravailler\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        env.render() # Si on veux le rendu\n",
    "        state = tf.convert_to_tensor(state)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "            \n",
    "            # pour l'état présent, renvoie les sorties des deux 2N Actor et Critic\n",
    "            # stocke l'avantage pour des calculs ultérieurs hors de la boucle\n",
    "        action_probs, critic_value = model(state)\n",
    "            # critic_value_history.append(critic_value[0,0])\n",
    "            # print(action_probs)\n",
    "            # print(critic_value)\n",
    "            # Policy : donne l'action selon les actions possibles et la distribution d'actions donnée par l'actor\n",
    "            # puis stocke cette action dans l'historique\n",
    "        action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            # action_probs_history.append(tf.math.log(action_probs[0,action]))\n",
    "            \n",
    "            # Joue l'environnement pour l'état présent et l'action précédemment choisie\n",
    "            # puis stocke la récompense dans l'historique\n",
    "            # Si done, alors le pendule est tombé et la simulation n'est plus intéressante.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_history.append(reward)\n",
    "            # episode_reward += reward\n",
    "            \n",
    "            # if done:\n",
    "                # break \n",
    "                \n",
    "            # print(timestep)]\n",
    "    env.close()\n",
    "    print(\"L'environnement Cartpole a terminé son exécution de {} itérations\".format(max_steps_per_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdc986-be57-47a0-9515-8c88945238eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
