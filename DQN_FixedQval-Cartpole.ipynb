{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fiscal-preservation",
   "metadata": {},
   "source": [
    "## Implémentation de DQN pour l'environnement Gym CartPole avec l'architecture Fixed Q values \n",
    "\n",
    "Ce notebook est juste une copie manuelle du DQN proposé par [Aurélien Géron](https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb) pour maîtriser l'équilibre d'un pendule inversé. Il vise uniquement à s'approprier les techniques de codage pour avoir une base performante avant de complexifier l'environnement et la tâche à réaliser par l'agent. \n",
    "\n",
    "A noter que l'affichage qui permet en général d'attester la qualité et la performance d'un DQN présente la somme des récompenses par épisode et non la loss qui n'est pas représentative de la qualité de l'apprentissage. Dans ce notebook, on explore la technique Fixed Q values qui consistent à créer deux modèles : un pour chaque côté de l'équation de Bellman target et training. On entraîne  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "friendly-haiti",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wireless-frontier",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "input_shape=[4]\n",
    "n_outputs=2\n",
    "\n",
    "def def_model(input_nb, output_nb):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(64, activation=\"elu\", input_shape=input_nb),\n",
    "        keras.layers.Dense(32, activation=\"elu\"),\n",
    "        keras.layers.Dense(output_nb)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prompt-companion",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, nb_ouputs, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(nb_ouputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endangered-therapy",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demanding-capability",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "infinite-treaty",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, n_outputs, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stylish-portland",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "gamma = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + (1-dones) * gamma * max_next_Q_values)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-kelly",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "maxi=0\n",
    "\n",
    "for episode in tqdm(range(600)):\n",
    "    obs = env.reset()\n",
    "    sum=0\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode/500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "        sum+=reward\n",
    "    rewards.append(sum)\n",
    "    \n",
    "    if sum>50 and sum>maxi:\n",
    "        model.save('cartpole')\n",
    "        maxi=sum\n",
    "    \n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "print('maxi = {}'.format(maxi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-scroll",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78761b3-cace-4b13-a4f8-709c4b9396a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,466\n",
      "Trainable params: 2,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "model = tf.keras.models.load_model('cartpole')\n",
    "\n",
    "# Check its architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d7da3f-1829-4fc1-9a63-de54a542bdd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:25<00:00, 19.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "for i in tqdm(range(500)):\n",
    "    env.render()\n",
    "    Q_values = model.predict(state[np.newaxis])\n",
    "    state, _, done, _ = env.step(np.argmax(Q_values[0])) # take a random action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c501fdb-e2b4-4388-9a90-7c950f64014e",
   "metadata": {},
   "source": [
    "import gym\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "for i in tqdm(range(500)):\n",
    "    Q_values = model.predict(state[np.newaxis])\n",
    "    state, _, done, _ = env.step(np.argmax(Q_values[0])) # take a random action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
